{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data exploration, preprocessing and feature engineering</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this and the following notebooks we will demonstrate how you can build your ML Pipeline leveraging SKLearn Feature Transformers and SageMaker XGBoost algorithm & after the model is trained, deploy the Pipeline (Feature Transformer and XGBoost) as a SageMaker Inference Pipeline behind a single Endpoint for real-time inference.\n",
    "\n",
    "In particular, in this notebook we will tackle the first steps related to data exploration and preparation. We will use [Amazon Athena](https://aws.amazon.com/athena/) to query our dataset and have a first insight about data quality and available features, [AWS Glue](https://aws.amazon.com/glue/) to create a Data Catalog and [Amazon SageMaker Processing](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) for building the feature transformer model with SKLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using Amazon SageMaker Studio, please set this variable to True and execute the cell\n",
    "use_sm_studio = False\n",
    "if use_sm_studio:\n",
    "    %cd /root/end-to-end-ml-sm/02_data_exploration_and_feature_eng/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n",
      "arn:aws:iam::208480242416:role/service-role/AmazonSageMaker-ExecutionRole-endtoendml\n",
      "sagemaker-us-east-1-208480242416\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "prefix = 'endtoendmlsm'\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now copy to our bucket the dataset used for this use case. We will use the `windturbine_raw_data.csv` made available for this workshop in the `gianpo-public` public S3 bucket. In this Notebook, we will download from that bucket and upload to your bucket so that AWS services can access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "#file_key = 'data/raw/windturbine_raw_data.csv'\n",
    "file_key = 'data/raw/windturbine_raw_data_header.csv'\n",
    "copy_source = {\n",
    "    'Bucket': 'gianpo-public',\n",
    "    'Key': 'endtoendml/{0}'.format(file_key)\n",
    "}\n",
    "\n",
    "s3.Bucket(bucket_name).copy(copy_source, '{0}/'.format(prefix) + file_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need now is to infer a schema for our dataset. Thanks to its [integration with AWS Glue](https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html), we will later use Amazon Athena to run SQL queries against our data stored in S3 without the need to import them into a relational database. To do so, Amazon Athena uses the AWS Glue Data Catalog as a central location to store and retrieve table metadata throughout an AWS account. The Athena execution engine, indeed, requires table metadata that instructs it where to read data, how to read it, and other information necessary to process the data.\n",
    "\n",
    "To organize our Glue Data Catalog we create a new database named `endtoendml-db`. To do so, we create a Glue client via Boto and invoke the `create_database` method.\n",
    "\n",
    "However, first we want to make sure these AWS resources to not exist yet to avoid any error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup completed.\n"
     ]
    }
   ],
   "source": [
    "from notebook_utilities import cleanup_glue_resources\n",
    "cleanup_glue_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client('glue')\n",
    "\n",
    "response = glue_client.create_database(DatabaseInput={'Name': 'endtoendml-db'})\n",
    "response = glue_client.get_database(Name='endtoendml-db')\n",
    "response\n",
    "assert response['Database']['Name'] == 'endtoendml-db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a Glue Crawler that we point to the S3 path where the dataset resides, and the crawler creates table definitions in the Data Catalog.\n",
    "To grant the correct set of access permission to the crawler, we use one of the roles created before (`GlueServiceRole-endtoendml`) whose policy grants AWS Glue access to data stored in your S3 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue_client.create_crawler(\n",
    "    Name='endtoendml-crawler',\n",
    "    Role='service-role/GlueServiceRole-endtoendml', \n",
    "    DatabaseName='endtoendml-db',\n",
    "    Targets={'S3Targets': [{'Path': '{0}/{1}/data/raw/'.format(bucket_name, prefix)}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to run the crawler with the `start_crawler` API and to monitor its status upon completion through the `get_crawler_metrics` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n"
     ]
    }
   ],
   "source": [
    "glue_client.start_crawler(Name='endtoendml-crawler')\n",
    "\n",
    "while glue_client.get_crawler_metrics(CrawlerNameList=['endtoendml-crawler'])['CrawlerMetricsList'][0]['TablesCreated'] == 0:\n",
    "    print('RUNNING')\n",
    "    time.sleep(15)\n",
    "    \n",
    "assert glue_client.get_crawler_metrics(CrawlerNameList=['endtoendml-crawler'])['CrawlerMetricsList'][0]['TablesCreated'] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the crawler has finished its job, we can retrieve the Table definition for the newly created table.\n",
    "As you can see, the crawler has been able to correctly identify 12 fields, infer a type for each column and assign a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table': {'Name': 'raw',\n",
       "  'DatabaseName': 'endtoendml-db',\n",
       "  'Owner': 'owner',\n",
       "  'CreateTime': datetime.datetime(2020, 4, 8, 22, 18, 51, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2020, 4, 8, 22, 18, 51, tzinfo=tzlocal()),\n",
       "  'LastAccessTime': datetime.datetime(2020, 4, 8, 22, 18, 51, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'turbine_id', 'Type': 'string'},\n",
       "    {'Name': 'turbine_type', 'Type': 'string'},\n",
       "    {'Name': 'wind_speed', 'Type': 'bigint'},\n",
       "    {'Name': 'rpm_blade', 'Type': 'bigint'},\n",
       "    {'Name': 'oil_temperature', 'Type': 'double'},\n",
       "    {'Name': 'oil_level', 'Type': 'bigint'},\n",
       "    {'Name': 'temperature', 'Type': 'bigint'},\n",
       "    {'Name': 'humidity', 'Type': 'bigint'},\n",
       "    {'Name': 'vibrations_frequency', 'Type': 'bigint'},\n",
       "    {'Name': 'pressure', 'Type': 'bigint'},\n",
       "    {'Name': 'wind_direction', 'Type': 'string'},\n",
       "    {'Name': 'breakdown', 'Type': 'string'}],\n",
       "   'Location': 's3://sagemaker-us-east-1-208480242416/endtoendmlsm/data/raw/',\n",
       "   'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
       "   'Compressed': False,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',\n",
       "    'Parameters': {'field.delim': ','}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'CrawlerSchemaSerializerVersion': '1.0',\n",
       "    'UPDATED_BY_CRAWLER': 'endtoendml-crawler',\n",
       "    'areColumnsQuoted': 'false',\n",
       "    'averageRecordSize': '75',\n",
       "    'classification': 'csv',\n",
       "    'columnsOrdered': 'true',\n",
       "    'compressionType': 'none',\n",
       "    'delimiter': ',',\n",
       "    'objectCount': '1',\n",
       "    'recordCount': '564265',\n",
       "    'sizeKey': '42319941',\n",
       "    'skip.header.line.count': '1',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'CrawlerSchemaSerializerVersion': '1.0',\n",
       "   'UPDATED_BY_CRAWLER': 'endtoendml-crawler',\n",
       "   'areColumnsQuoted': 'false',\n",
       "   'averageRecordSize': '75',\n",
       "   'classification': 'csv',\n",
       "   'columnsOrdered': 'true',\n",
       "   'compressionType': 'none',\n",
       "   'delimiter': ',',\n",
       "   'objectCount': '1',\n",
       "   'recordCount': '564265',\n",
       "   'sizeKey': '42319941',\n",
       "   'skip.header.line.count': '1',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:sts::208480242416:assumed-role/GlueServiceRole-endtoendml/AWS-Crawler',\n",
       "  'IsRegisteredWithLakeFormation': False},\n",
       " 'ResponseMetadata': {'RequestId': '7421af61-9f8a-436a-a692-c7682deb7028',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Wed, 08 Apr 2020 22:19:02 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2093',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '7421af61-9f8a-436a-a692-c7682deb7028'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = glue_client.get_table(DatabaseName='endtoendml-db', Name='raw')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our knowledge of the dataset, we can assign more specific names to columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '23a4052b-0ee2-4998-83f1-5c15f18cec39',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Wed, 08 Apr 2020 22:19:02 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '23a4052b-0ee2-4998-83f1-5c15f18cec39'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['Table']['StorageDescriptor']['Columns'] = [{'Name': 'turbine_id', 'Type': 'string'},\n",
    "                                                  {'Name': 'turbine_type', 'Type': 'string'},\n",
    "                                                  {'Name': 'wind_speed', 'Type': 'double'},\n",
    "                                                  {'Name': 'rpm_blade', 'Type': 'double'},\n",
    "                                                  {'Name': 'oil_temperature', 'Type': 'double'},\n",
    "                                                  {'Name': 'oil_level', 'Type': 'double'},\n",
    "                                                  {'Name': 'temperature', 'Type': 'double'},\n",
    "                                                  {'Name': 'humidity', 'Type': 'double'},\n",
    "                                                  {'Name': 'vibrations_frequency', 'Type': 'double'},\n",
    "                                                  {'Name': 'pressure', 'Type': 'double'},\n",
    "                                                  {'Name': 'wind_direction', 'Type': 'string'},\n",
    "                                                  {'Name': 'breakdown', 'Type': 'string'}]\n",
    "updated_table = table['Table']\n",
    "updated_table.pop('DatabaseName', None)\n",
    "updated_table.pop('CreateTime', None)\n",
    "updated_table.pop('UpdateTime', None)\n",
    "updated_table.pop('CreatedBy', None)\n",
    "updated_table.pop('IsRegisteredWithLakeFormation', None)\n",
    "\n",
    "glue_client.update_table(\n",
    "    DatabaseName='endtoendml-db',\n",
    "    TableInput=updated_table\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data exploration with Amazon Athena</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data exploration, let's install PyAthena, a Python client for Amazon Athena. Note: PyAthena is not maintained by AWS, please visit: https://pypi.org/project/PyAthena/ for additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.1.5)\n",
      "Requirement already satisfied: botocore in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3fs) (1.15.24)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3fs) (1.12.24)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore->s3fs) (0.9.4)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore->s3fs) (1.23)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore->s3fs) (2.7.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore->s3fs) (0.14)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->s3fs) (0.3.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore->s3fs) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pyathena in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.10.3)\n",
      "Requirement already satisfied: boto3>=1.4.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (1.12.24)\n",
      "Requirement already satisfied: botocore>=1.5.52 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (1.15.24)\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (6.1.0)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (0.18.2)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.4.4->pyathena) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.4.4->pyathena) (0.9.4)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (2.7.3)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (1.23)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tenacity>=4.1.0->pyathena) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install s3fs\n",
    "!pip install pyathena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turbine_id</th>\n",
       "      <th>turbine_type</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>rpm_blade</th>\n",
       "      <th>oil_temperature</th>\n",
       "      <th>oil_level</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>vibrations_frequency</th>\n",
       "      <th>pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>breakdown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TID003</td>\n",
       "      <td>HAWT</td>\n",
       "      <td>80.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>E</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TID010</td>\n",
       "      <td>HAWT</td>\n",
       "      <td>85.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NE</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TID007</td>\n",
       "      <td>HAWT</td>\n",
       "      <td>47.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>N</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TID008</td>\n",
       "      <td>VAWT</td>\n",
       "      <td>73.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>SW</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TID003</td>\n",
       "      <td>HAWT</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TID001</td>\n",
       "      <td>HAWT</td>\n",
       "      <td>78.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>SW</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TID009</td>\n",
       "      <td>HAWT</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TID002</td>\n",
       "      <td>VAWT</td>\n",
       "      <td>59.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  turbine_id turbine_type  wind_speed  rpm_blade  oil_temperature  oil_level  \\\n",
       "0     TID003         HAWT        80.0       61.0              NaN       34.0   \n",
       "1     TID010         HAWT        85.0       78.0             36.0       28.0   \n",
       "2     TID007         HAWT        47.0       31.0             31.0       23.0   \n",
       "3     TID008         VAWT        73.0       70.0             38.0        8.0   \n",
       "4     TID003         HAWT        16.0       23.0             46.0        9.0   \n",
       "5     TID001         HAWT        78.0       71.0             30.0       11.0   \n",
       "6     TID009         HAWT        80.0       25.0             37.0       31.0   \n",
       "7     TID002         VAWT        59.0       29.0             37.0       10.0   \n",
       "\n",
       "   temperature  humidity  vibrations_frequency  pressure wind_direction  \\\n",
       "0         33.0      26.0                   1.0      77.0              E   \n",
       "1         35.0      43.0                  15.0      62.0             NE   \n",
       "2         46.0      62.0                  15.0      32.0              N   \n",
       "3         17.0      66.0                   6.0      80.0             SW   \n",
       "4         76.0      53.0                  14.0      29.0              W   \n",
       "5         66.0      79.0                   1.0      81.0             SW   \n",
       "6         40.0      75.0                   4.0      56.0             NW   \n",
       "7         25.0      83.0                  13.0      55.0             SE   \n",
       "\n",
       "  breakdown  \n",
       "0        no  \n",
       "1       yes  \n",
       "2        no  \n",
       "3       yes  \n",
       "4        no  \n",
       "5        no  \n",
       "6        no  \n",
       "7        no  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyathena\n",
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "\n",
    "athena_cursor = connect(s3_staging_dir='s3://{0}/{1}/staging/'.format(bucket_name, prefix), \n",
    "                        region_name=region).cursor()\n",
    "\n",
    "athena_cursor.execute('SELECT * FROM \"endtoendml-db\".raw limit 8;')\n",
    "pd.read_csv(athena_cursor.output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another SQL query to count how many records we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_col0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _col0\n",
       "0  1000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athena_cursor.execute('SELECT COUNT(*) FROM \"endtoendml-db\".raw;')\n",
    "pd.read_csv(athena_cursor.output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to see what are possible values for the field \"breakdown\" and how frequently they occur over the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>breakdown</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>86.3421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes</td>\n",
       "      <td>13.6579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  breakdown  percent\n",
       "0        no  86.3421\n",
       "1       yes  13.6579"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athena_cursor.execute('SELECT breakdown, (COUNT(breakdown) * 100.0 / (SELECT COUNT(*) FROM \"endtoendml-db\".raw)) \\\n",
    "            AS percent FROM \"endtoendml-db\".raw GROUP BY breakdown;')\n",
    "pd.read_csv(athena_cursor.output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "athena_cursor.execute('SELECT breakdown, COUNT(breakdown) AS bd_count FROM \"endtoendml-db\".raw GROUP BY breakdown;')\n",
    "df = pd.read_csv(athena_cursor.output_location)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(df.breakdown, df.bd_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have discovered that the dataset is quite unbalanced, although we are not going to try balancing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turbine_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAWT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VAWT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  turbine_type\n",
       "0          NaN\n",
       "1         HAWT\n",
       "2         VAWT"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athena_cursor.execute('SELECT DISTINCT(turbine_type) FROM \"endtoendml-db\".raw')\n",
    "pd.read_csv(athena_cursor.output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_col0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _col0\n",
       "0  38297"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athena_cursor.execute('SELECT COUNT(*) FROM \"endtoendml-db\".raw WHERE oil_temperature IS NULL GROUP BY oil_temperature')\n",
    "pd.read_csv(athena_cursor.output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also realized there are a few null values that need to be managed during the data preparation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of keeping the data exploration step short during the workshop, we are not going to execute additional queries. However, feel free to explore the dataset more if you have time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: you can go to Amazon Athena console and check for query duration under History tab: usually queries are executed in a few seconds, then it some time for Pandas to load results into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started with preprocessing and feature engineering, we want to leverage on Amazon SageMaker Experiments to track the experimentations that we will be executing.\n",
    "We are going to create a new experiment and then a new trial, that represents a multi-step ML workflow (e.g. preprocessing stage1, preprocessing stage2, training stage, etc.). Each step of a trial maps to a trial component in SageMaker Experiments.\n",
    "\n",
    "We will use the Amazon SageMaker Experiments SDK to interact with the service from the notebooks. Additional info and documentation is available here: https://github.com/aws/sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker-experiments in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.1.12)\n",
      "Requirement already satisfied: boto3>=1.12.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker-experiments) (1.12.24)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.24 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments) (1.15.24)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments) (0.9.4)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.24->boto3>=1.12.8->sagemaker-experiments) (1.23)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.24->boto3>=1.12.8->sagemaker-experiments) (2.7.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.24->boto3>=1.12.8->sagemaker-experiments) (0.14)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.24->boto3>=1.12.8->sagemaker-experiments) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are creating the experiment, or loading if it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment loaded.\n"
     ]
    }
   ],
   "source": [
    "from smexperiments import experiment\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "experiment_name = 'end-to-end-ml-sagemaker'\n",
    "current_experiment = None\n",
    "\n",
    "try:\n",
    "    current_experiment = experiment.Experiment.load(experiment_name)\n",
    "    print('Experiment loaded.')\n",
    "except ClientError as ex:\n",
    "    if ex.response['Error']['Code'] == 'ResourceNotFound':\n",
    "        # Create experiment\n",
    "        current_experiment = experiment.Experiment.create(experiment_name=experiment_name,\n",
    "                                                          description='SageMaker workshop experiment')\n",
    "        print('Experiment created.')\n",
    "    else:\n",
    "        raise ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our experiment, we can create a new trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = current_experiment.create_trial(trial_name='sklearn-xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now own, we will use the experiment and the trial as configuration parameters for the preprocessing and training jobs, to make sure we track executions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocessing and Feature Engineering with Amazon SageMaker Processing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing and feature engineering code is implemented in the `source_dir/preprocessor.py` file.\n",
    "\n",
    "You can go through the code and see that a few categorical columns required one-hot encoding, plus we are filling some NaN values based on domain knowledge.\n",
    "Once the SKLearn fit() and transform() is done, we are splitting our dataset into 80/20 train & validation and then saving to the output paths whose content will be automatically uploaded to Amazon S3 by SageMaker Processing. Finally, we also save the featurizer model as it will be reused later for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mwarnings\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "subprocess.call([\u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-experiments\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msmexperiments.tracker\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Tracker\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn.externals\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m joblib\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn.model_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn.preprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m OneHotEncoder, LabelEncoder\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn.compose\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m make_column_transformer\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn.exceptions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DataConversionWarning\r\n",
      "warnings.filterwarnings(action=\u001b[33m'\u001b[39;49;00m\u001b[33mignore\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, category=DataConversionWarning)\r\n",
      "\r\n",
      "columns = [\u001b[33m'\u001b[39;49;00m\u001b[33mturbine_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mturbine_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwind_speed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mrpm_blade\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33moil_temperature\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "           \u001b[33m'\u001b[39;49;00m\u001b[33moil_level\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtemperature\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mhumidity\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mvibrations_frequency\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpressure\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwind_direction\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mbreakdown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m==\u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    \r\n",
      "    \u001b[37m# Read the arguments passed to the script.\u001b[39;49;00m\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train-test-split-ratio\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.3\u001b[39;49;00m)\r\n",
      "    args, _ = parser.parse_known_args()\r\n",
      "    \r\n",
      "    \u001b[37m# Tracking specific parameter value during job.\u001b[39;49;00m\r\n",
      "    tracker = Tracker.load()\r\n",
      "    tracker.log_parameter(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain-test-split-ratio\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, args.train_test_split_ratio)\r\n",
      "    \r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mReceived arguments {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args))\r\n",
      "\r\n",
      "    \u001b[37m# Read input data into a Pandas dataframe.\u001b[39;49;00m\r\n",
      "    input_data_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwindturbine_raw_data_header.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mReading input data from {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(input_data_path))\r\n",
      "    df = pd.read_csv(input_data_path)\r\n",
      "    df.columns = columns\r\n",
      "    \r\n",
      "    \u001b[37m# Replacing certain null values.\u001b[39;49;00m\r\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33mturbine_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = df[\u001b[33m'\u001b[39;49;00m\u001b[33mturbine_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].fillna(\u001b[33m\"\u001b[39;49;00m\u001b[33mHAWT\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    tracker.log_parameter(\u001b[33m'\u001b[39;49;00m\u001b[33mdefault-turbine-type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mHAWT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33moil_temperature\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = df[\u001b[33m'\u001b[39;49;00m\u001b[33moil_temperature\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].fillna(\u001b[34m37.0\u001b[39;49;00m)\r\n",
      "    tracker.log_parameter(\u001b[33m'\u001b[39;49;00m\u001b[33mdefault-oil-temperature\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m37.0\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m# Defining one-hot encoders.\u001b[39;49;00m\r\n",
      "    transformer = make_column_transformer(\r\n",
      "        ([\u001b[33m'\u001b[39;49;00m\u001b[33mturbine_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mturbine_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwind_direction\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], OneHotEncoder(sparse=\u001b[36mFalse\u001b[39;49;00m)), remainder=\u001b[33m\"\u001b[39;49;00m\u001b[33mpassthrough\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    \r\n",
      "    X = df.drop(\u001b[33m'\u001b[39;49;00m\u001b[33mbreakdown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis=\u001b[34m1\u001b[39;49;00m)\r\n",
      "    y = df[\u001b[33m'\u001b[39;49;00m\u001b[33mbreakdown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \r\n",
      "    featurizer_model = transformer.fit(X)\r\n",
      "    features = featurizer_model.transform(X)\r\n",
      "    labels = LabelEncoder().fit_transform(y)\r\n",
      "    \r\n",
      "    \u001b[37m# Splitting.\u001b[39;49;00m\r\n",
      "    split_ratio = args.train_test_split_ratio\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSplitting data into train and validation sets with ratio {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(split_ratio))\r\n",
      "    X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=split_ratio, random_state=\u001b[34m0\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTrain features shape after preprocessing: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(X_train.shape))\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mValidation features shape after preprocessing: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(X_val.shape))\r\n",
      "    \r\n",
      "    \u001b[37m# Saving outputs.\u001b[39;49;00m\r\n",
      "    train_features_output_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain_features.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    train_labels_output_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain_labels.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    val_features_output_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/val\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mval_features.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    val_labels_output_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/val\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mval_labels.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving training features to {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_features_output_path))\r\n",
      "    pd.DataFrame(X_train).to_csv(train_features_output_path, header=\u001b[36mFalse\u001b[39;49;00m, index=\u001b[36mFalse\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving validation features to {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(val_features_output_path))\r\n",
      "    pd.DataFrame(X_val).to_csv(val_features_output_path, header=\u001b[36mFalse\u001b[39;49;00m, index=\u001b[36mFalse\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving training labels to {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_labels_output_path))\r\n",
      "    pd.DataFrame(y_train).to_csv(train_labels_output_path, header=\u001b[36mFalse\u001b[39;49;00m, index=\u001b[36mFalse\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving validation labels to {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(val_labels_output_path))\r\n",
      "    pd.DataFrame(y_val).to_csv(val_labels_output_path, header=\u001b[36mFalse\u001b[39;49;00m, index=\u001b[36mFalse\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m# Saving model.\u001b[39;49;00m\r\n",
      "    model_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.joblib\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    model_output_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.tar.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving featurizer model to {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(model_output_path))\r\n",
      "    joblib.dump(featurizer_model, model_path)\r\n",
      "    tar = tarfile.open(model_output_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw:gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    tar.add(model_path, arcname=\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.joblib\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    tar.close()\r\n",
      "    \r\n",
      "    tracker.close()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize source_dir/preprocessor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring an Amazon SageMaker Processing job through the SM Python SDK requires to create a `Processor` object (in this case `SKLearnProcessor` as we are using the default SKLearn container for processing); we can specify how many instances we are going to use and what instance type is requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(role=role,\n",
    "                                     base_job_name='end-to-end-ml-sm-proc',\n",
    "                                     instance_type='ml.m5.large',\n",
    "                                     instance_count=1,\n",
    "                                     framework_version='0.20.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can invoke the `run()` method of the `Processor` object to kick-off the job, specifying the script to execute, its arguments and the configuration of inputs and outputs as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name end-to-end-ml-sm-proc-2020-04-08-22-36-17-292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  end-to-end-ml-sm-proc-2020-04-08-22-36-17-292\n",
      "Inputs:  [{'InputName': 'raw_data', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-208480242416/endtoendmlsm/data/raw/', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-208480242416/end-to-end-ml-sm-proc-2020-04-08-22-36-17-292/input/code/preprocessor.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-208480242416/endtoendmlsm/data/preprocessed/train/', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'val_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-208480242416/endtoendmlsm/data/preprocessed/val/', 'LocalPath': '/opt/ml/processing/val', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'model', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-208480242416/endtoendmlsm/output/sklearn/', 'LocalPath': '/opt/ml/processing/model', 'S3UploadMode': 'EndOfJob'}}]\n",
      "......................\u001b[34mCollecting sagemaker-experiments\n",
      "  Downloading https://files.pythonhosted.org/packages/5f/17/27df3daa7853a54f673deea7f3e5baf852afabfd8220146fbb5ab0ef236c/sagemaker_experiments-0.1.12-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mCollecting boto3>=1.12.8\n",
      "  Downloading https://files.pythonhosted.org/packages/27/87/de75e5a24584d82cca60b86f95d06e56412ed9e23807dcf23896f206f58e/boto3-1.12.39-py2.py3-none-any.whl (128kB)\u001b[0m\n",
      "\u001b[34mCollecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\u001b[0m\n",
      "\u001b[34mCollecting botocore<1.16.0,>=1.15.39\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/fb/f78a0e09965c156fea9160713705af688ec4f18af4249b3095949c930f77/botocore-1.15.39-py2.py3-none-any.whl (6.1MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /miniconda3/lib/python3.7/site-packages (from boto3>=1.12.8->sagemaker-experiments) (0.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /miniconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.39->boto3>=1.12.8->sagemaker-experiments) (1.24.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /miniconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.39->boto3>=1.12.8->sagemaker-experiments) (2.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils<0.16,>=0.10 in /miniconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.39->boto3>=1.12.8->sagemaker-experiments) (0.15.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /miniconda3/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.39->boto3>=1.12.8->sagemaker-experiments) (1.12.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: botocore, s3transfer, boto3, sagemaker-experiments\n",
      "  Found existing installation: botocore 1.13.6\n",
      "    Uninstalling botocore-1.13.6:\n",
      "      Successfully uninstalled botocore-1.13.6\n",
      "  Found existing installation: s3transfer 0.2.1\n",
      "    Uninstalling s3transfer-0.2.1:\n",
      "      Successfully uninstalled s3transfer-0.2.1\n",
      "  Found existing installation: boto3 1.10.6\n",
      "    Uninstalling boto3-1.10.6:\n",
      "      Successfully uninstalled boto3-1.10.6\u001b[0m\n",
      "\u001b[34mSuccessfully installed boto3-1.12.39 botocore-1.15.39 s3transfer-0.3.3 sagemaker-experiments-0.1.12\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mReceived arguments Namespace(train_test_split_ratio=0.2)\u001b[0m\n",
      "\u001b[34mReading input data from /opt/ml/processing/input/windturbine_raw_data_header.csv\u001b[0m\n",
      "\u001b[34mSplitting data into train and validation sets with ratio 0.2\u001b[0m\n",
      "\u001b[34mTrain features shape after preprocessing: (800000, 28)\u001b[0m\n",
      "\u001b[34mValidation features shape after preprocessing: (200000, 28)\u001b[0m\n",
      "\u001b[34mSaving training features to /opt/ml/processing/train/train_features.csv\u001b[0m\n",
      "\u001b[34mSaving validation features to /opt/ml/processing/val/val_features.csv\u001b[0m\n",
      "\u001b[34mSaving training labels to /opt/ml/processing/train/train_labels.csv\u001b[0m\n",
      "\u001b[34mSaving validation labels to /opt/ml/processing/val/val_labels.csv\u001b[0m\n",
      "\u001b[34mSaving featurizer model to /opt/ml/processing/model/model.tar.gz\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data_path = 's3://{0}/{1}/data/raw/'.format(bucket_name, prefix)\n",
    "train_data_path = 's3://{0}/{1}/data/preprocessed/train/'.format(bucket_name, prefix)\n",
    "val_data_path = 's3://{0}/{1}/data/preprocessed/val/'.format(bucket_name, prefix)\n",
    "model_path = 's3://{0}/{1}/output/sklearn/'.format(bucket_name, prefix)\n",
    "\n",
    "# Experiment tracking configuration\n",
    "experiment_config={\n",
    "    \"ExperimentName\": current_experiment.experiment_name,\n",
    "    \"TrialName\": current_trial.trial_name,\n",
    "    \"TrialComponentDisplayName\": \"sklearn-preprocessing\",\n",
    "}\n",
    "\n",
    "sklearn_processor.run(code='source_dir/preprocessor.py',\n",
    "                      inputs=[ProcessingInput(input_name='raw_data', source=raw_data_path, destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data', source='/opt/ml/processing/train', destination=train_data_path),\n",
    "                               ProcessingOutput(output_name='val_data', source='/opt/ml/processing/val', destination=val_data_path),\n",
    "                               ProcessingOutput(output_name='model', source='/opt/ml/processing/model', destination=model_path)],\n",
    "                      arguments=['--train-test-split-ratio', '0.2'],\n",
    "                      experiment_config=experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job is completed, we can give a look at the preprocessed dataset, by loading the validation features as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'val_features.csv'\n",
    "s3_key_prefix = '{0}/data/preprocessed/val/{1}'.format(prefix, file_name)\n",
    "\n",
    "sagemaker_session.download_data('./', bucket_name, s3_key_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0.1</th>\n",
       "      <th>0.0.2</th>\n",
       "      <th>0.0.3</th>\n",
       "      <th>1.0</th>\n",
       "      <th>0.0.4</th>\n",
       "      <th>0.0.5</th>\n",
       "      <th>0.0.6</th>\n",
       "      <th>0.0.7</th>\n",
       "      <th>0.0.8</th>\n",
       "      <th>...</th>\n",
       "      <th>0.0.15</th>\n",
       "      <th>0.0.16</th>\n",
       "      <th>25.0</th>\n",
       "      <th>30.0</th>\n",
       "      <th>30.0.1</th>\n",
       "      <th>17.0</th>\n",
       "      <th>38.0</th>\n",
       "      <th>16.0</th>\n",
       "      <th>1.0.3</th>\n",
       "      <th>20.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0.0  0.0.1  0.0.2  0.0.3  1.0  0.0.4  0.0.5  0.0.6  0.0.7  0.0.8  ...  \\\n",
       "0  0.0    1.0    0.0    0.0  0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "1  0.0    0.0    0.0    0.0  1.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "2  0.0    0.0    0.0    0.0  0.0    0.0    0.0    1.0    0.0    0.0  ...   \n",
       "3  0.0    0.0    0.0    0.0  0.0    0.0    0.0    1.0    0.0    0.0  ...   \n",
       "4  0.0    0.0    0.0    0.0  0.0    0.0    0.0    0.0    1.0    0.0  ...   \n",
       "5  0.0    0.0    0.0    0.0  0.0    0.0    1.0    0.0    0.0    0.0  ...   \n",
       "6  0.0    0.0    1.0    0.0  0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "7  0.0    0.0    0.0    0.0  0.0    0.0    0.0    0.0    1.0    0.0  ...   \n",
       "8  0.0    0.0    0.0    0.0  1.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "9  0.0    0.0    0.0    0.0  0.0    0.0    0.0    0.0    0.0    1.0  ...   \n",
       "\n",
       "   0.0.15  0.0.16  25.0  30.0  30.0.1  17.0  38.0  16.0  1.0.3  20.0  \n",
       "0     0.0     0.0  37.0  43.0    41.0  26.0  65.0  71.0   10.0  76.0  \n",
       "1     0.0     0.0  22.0  35.0    35.0  22.0  85.0  52.0    3.0  22.0  \n",
       "2     0.0     0.0  66.0  41.0    37.0  31.0  37.0  59.0    2.0  30.0  \n",
       "3     0.0     0.0  19.0  48.0    49.0   7.0  84.0  38.0    3.0  51.0  \n",
       "4     0.0     0.0  65.0  65.0    48.0  17.0  27.0  33.0    2.0  40.0  \n",
       "5     1.0     0.0  63.0  26.0    26.0  10.0  50.0  78.0   13.0  59.0  \n",
       "6     0.0     0.0  79.0  46.0    35.0  11.0  40.0  38.0    2.0  59.0  \n",
       "7     0.0     0.0  16.0  48.0    45.0  16.0  17.0  21.0    2.0  49.0  \n",
       "8     0.0     0.0  83.0  53.0    38.0  20.0  81.0  46.0    5.0  62.0  \n",
       "9     0.0     0.0  38.0  83.0    30.0  35.0  43.0  77.0    4.0  34.0  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "df.head(10)\n",
    "#print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the categorical variables have been one-hot encoded, and you are free to check that we do not have NaN values anymore as expected.\n",
    "Note that exploring the dataset locally with Pandas vs using Amazon Athena is possible given the limited size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment analytics\n",
    "\n",
    "You can visualize experiment analytics either from Amazon SageMaker Studio Experiments plug-in or using the SDK from a notebook, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>SageMaker.InstanceCount</th>\n",
       "      <th>SageMaker.InstanceType</th>\n",
       "      <th>SageMaker.VolumeSizeInGB</th>\n",
       "      <th>default-oil-temperature</th>\n",
       "      <th>default-turbine-type</th>\n",
       "      <th>train-test-split-ratio</th>\n",
       "      <th>SageMaker.ImageUri</th>\n",
       "      <th>...</th>\n",
       "      <th>train:rmse - Avg</th>\n",
       "      <th>train:rmse - StdDev</th>\n",
       "      <th>train:rmse - Last</th>\n",
       "      <th>train:rmse - Count</th>\n",
       "      <th>validation:rmse - Min</th>\n",
       "      <th>validation:rmse - Max</th>\n",
       "      <th>validation:rmse - Avg</th>\n",
       "      <th>validation:rmse - StdDev</th>\n",
       "      <th>validation:rmse - Last</th>\n",
       "      <th>validation:rmse - Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>end-to-end-ml-sm-proc-2020-04-08-22-36-17-292-...</td>\n",
       "      <td>sklearn-preprocessing</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:208480242416:proce...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ml.m5.large</td>\n",
       "      <td>30.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>HAWT</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>end-to-end-ml-sm-proc-2020-04-08-22-20-21-151-...</td>\n",
       "      <td>sklearn-preprocessing</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:208480242416:proce...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ml.m5.large</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>end-to-end-ml-sm-xgb-2020-04-08-11-34-36-607-a...</td>\n",
       "      <td>xgboost-training</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:208480242416:train...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ml.m5.xlarge</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>683313688378.dkr.ecr.us-east-1.amazonaws.com/s...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280832</td>\n",
       "      <td>0.086263</td>\n",
       "      <td>0.174321</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.174362</td>\n",
       "      <td>0.461911</td>\n",
       "      <td>0.280847</td>\n",
       "      <td>0.086218</td>\n",
       "      <td>0.174362</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>end-to-end-ml-sm-proc-2020-04-08-11-10-50-241-...</td>\n",
       "      <td>sklearn-preprocessing</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:208480242416:proce...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ml.m5.large</td>\n",
       "      <td>30.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>HAWT</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows  35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  TrialComponentName            DisplayName  \\\n",
       "0  end-to-end-ml-sm-proc-2020-04-08-22-36-17-292-...  sklearn-preprocessing   \n",
       "1  end-to-end-ml-sm-proc-2020-04-08-22-20-21-151-...  sklearn-preprocessing   \n",
       "2  end-to-end-ml-sm-xgb-2020-04-08-11-34-36-607-a...       xgboost-training   \n",
       "3  end-to-end-ml-sm-proc-2020-04-08-11-10-50-241-...  sklearn-preprocessing   \n",
       "\n",
       "                                           SourceArn  SageMaker.InstanceCount  \\\n",
       "0  arn:aws:sagemaker:us-east-1:208480242416:proce...                      1.0   \n",
       "1  arn:aws:sagemaker:us-east-1:208480242416:proce...                      1.0   \n",
       "2  arn:aws:sagemaker:us-east-1:208480242416:train...                      1.0   \n",
       "3  arn:aws:sagemaker:us-east-1:208480242416:proce...                      1.0   \n",
       "\n",
       "  SageMaker.InstanceType  SageMaker.VolumeSizeInGB  default-oil-temperature  \\\n",
       "0            ml.m5.large                      30.0                     37.0   \n",
       "1            ml.m5.large                      30.0                      NaN   \n",
       "2           ml.m5.xlarge                      30.0                      NaN   \n",
       "3            ml.m5.large                      30.0                     37.0   \n",
       "\n",
       "  default-turbine-type  train-test-split-ratio  \\\n",
       "0                 HAWT                     0.2   \n",
       "1                  NaN                     NaN   \n",
       "2                  NaN                     NaN   \n",
       "3                 HAWT                     0.2   \n",
       "\n",
       "                                  SageMaker.ImageUri  ... train:rmse - Avg  \\\n",
       "0                                                NaN  ...              NaN   \n",
       "1                                                NaN  ...              NaN   \n",
       "2  683313688378.dkr.ecr.us-east-1.amazonaws.com/s...  ...         0.280832   \n",
       "3                                                NaN  ...              NaN   \n",
       "\n",
       "  train:rmse - StdDev train:rmse - Last train:rmse - Count  \\\n",
       "0                 NaN               NaN                NaN   \n",
       "1                 NaN               NaN                NaN   \n",
       "2            0.086263          0.174321               20.0   \n",
       "3                 NaN               NaN                NaN   \n",
       "\n",
       "  validation:rmse - Min validation:rmse - Max  validation:rmse - Avg  \\\n",
       "0                   NaN                   NaN                    NaN   \n",
       "1                   NaN                   NaN                    NaN   \n",
       "2              0.174362              0.461911               0.280847   \n",
       "3                   NaN                   NaN                    NaN   \n",
       "\n",
       "  validation:rmse - StdDev validation:rmse - Last validation:rmse - Count  \n",
       "0                      NaN                    NaN                     NaN  \n",
       "1                      NaN                    NaN                     NaN  \n",
       "2                 0.086218               0.174362                    20.0  \n",
       "3                      NaN                    NaN                     NaN  \n",
       "\n",
       "[4 rows x 35 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "analytics = ExperimentAnalytics(experiment_name=experiment_name)\n",
    "analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the preprocessing and feature engineering are completed, you can move to the next notebook in the **03_train_model** folder to start model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
